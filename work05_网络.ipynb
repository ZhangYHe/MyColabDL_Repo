{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "work05 网络.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2XlE6Zd36S06xX1efHL01",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZhangYHe/MyColabDL_Repo/blob/main/work05_%E7%BD%91%E7%BB%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeoOk4jsi_J3"
      },
      "outputs": [],
      "source": [
        "class MyRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, **kwargs):\n",
        "        super(MyRNN, self).__init__(**kwargs)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "   \n",
        "        self.My_encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers, dropout=1)\n",
        "        self.My_decoder = nn.Linear(num_hiddens , 2)   # num_hiddens*2 ?\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # inputs [batchsize，time_step]\n",
        "        # inputs_T [time_step, batchsize]\n",
        "        # embeddings [time_step，batchsize，embed_size]\n",
        "        # outputs [time_step，batchsize, num_hiddens]\n",
        "\n",
        "        inputs_T=inputs.T\n",
        "        embeddings = self.embedding(inputs_T)\n",
        "        self.My_encoder.flatten_parameters()\n",
        "        \n",
        "        outputs, _ = self.My_encoder(embeddings)\n",
        "        # encoding = outputs\n",
        "        encoding = torch.cat((outputs[0], outputs[-1]), dim=1)\n",
        "        type = self.My_decoder(encoding)\n",
        "\n",
        "        return type"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "    if type(m) == nn.LSTM:\n",
        "        for param in m._flat_weights_names:\n",
        "            if \"weight\" in param:\n",
        "                nn.init.xavier_uniform_(m._parameters[param])"
      ],
      "metadata": {
        "id": "RKm9lH51ov1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_gpus():\n",
        "    devices = [torch.device(f'cuda:{i}')\n",
        "             for i in range(torch.cuda.device_count())]\n",
        "    return devices if devices else [torch.device('cpu')]\n"
      ],
      "metadata": {
        "id": "lFdJ6AIRpopD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size, num_hiddens, num_layers = 100, 100, 2\n",
        "devices = find_gpus()\n",
        "net = MyRNN(len(vocab), embed_size, num_hiddens, num_layers)\n",
        "\n",
        "net.apply(init_weights)"
      ],
      "metadata": {
        "id": "YaG2-B8RosO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##定义网络   \n",
        "网络框架如下所示：   \n",
        "> input: [batchsize，time_step]  \n",
        ">\n",
        "> Embedding   \n",
        ">> input: [batchsize，time_step]   \n",
        ">> output: [time_step，batchsize，embed_size]   \n",
        ">  \n",
        "> LSTM\n",
        ">> input: [time_step，batchsize，embed_size]  \n",
        ">> output: [time_step，batchsize, num_hiddens]   \n",
        "> \n",
        ">Drouout   \n",
        ">> input: [time_step，batchsize, num_hiddens]  \n",
        ">> output: [time_step，batchsize, num_hiddens]  \n",
        "> \n",
        "> Dense   \n",
        ">> input: [time_step，batchsize, num_hiddens]  \n",
        ">> output: [batchsize, class_num]   \n",
        ">   \n",
        "> output: [batchsize, class_num]  "
      ],
      "metadata": {
        "id": "9nhqnF8zqEw0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbdGgncqyNtw"
      },
      "outputs": [],
      "source": [
        "class MyRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, **kwargs):\n",
        "        super(MyRNN, self).__init__(**kwargs)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "   \n",
        "        self.My_encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers, dropout=0.5)\n",
        "        self.My_decoder = nn.Linear(num_hiddens*2 , 2)   # num_hiddens*2 ?\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # inputs [batchsize，time_step]\n",
        "        # inputs_T [time_step, batchsize]\n",
        "        # embeddings [time_step，batchsize，embed_size]\n",
        "        # outputs [time_step，batchsize, num_hiddens]\n",
        "\n",
        "        inputs_T=inputs.T\n",
        "        embeddings = self.embedding(inputs_T)\n",
        "        print(\"emb\",embeddings.shape)\n",
        "        self.My_encoder.flatten_parameters()\n",
        "        \n",
        "        outputs, _ = self.My_encoder(embeddings)\n",
        "        print(\"outputs\",outputs.shape)\n",
        "        # encoding = outputs\n",
        "        encoding = torch.cat((outputs[0], outputs[-1]), dim=1)\n",
        "\n",
        "        type = self.My_decoder(encoding)\n",
        "        print(\"type\",type.shape)\n",
        "        return type"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##初始化模型参数   \n",
        "对网络每一层的参数进行初始化"
      ],
      "metadata": {
        "id": "49x_MNBcslId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "    if type(m) == nn.LSTM:\n",
        "        for param in m._flat_weights_names:\n",
        "            if \"weight\" in param:\n",
        "                nn.init.xavier_uniform_(m._parameters[param])"
      ],
      "metadata": {
        "id": "yyCH5jExyNtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_gpus():\n",
        "    devices = [torch.device(f'cuda:{i}')\n",
        "             for i in range(torch.cuda.device_count())]\n",
        "    return devices if devices else [torch.device('cpu')]\n"
      ],
      "metadata": {
        "id": "zlk89Y_kyNtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size, num_hiddens, num_layers = 100, 100, 2\n",
        "batchsize, time_step = 5, 2\n",
        "devices = find_gpus()\n",
        "# net = MyRNN(len(vocab), embed_size, num_hiddens, num_layers)\n",
        "net = MyRNN(28, embed_size, num_hiddens, num_layers)  #测试\n",
        "\n",
        "net.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29979cd2-78f8-4c05-cefd-a1914ced1272",
        "id": "glyL2y5IyNtx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyRNN(\n",
              "  (embedding): Embedding(28, 100)\n",
              "  (My_encoder): LSTM(100, 100, num_layers=2, dropout=0.5)\n",
              "  (My_decoder): Linear(in_features=200, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#测试网络输出\n",
        "X = torch.arange(10).reshape((batchsize, time_step))\n",
        "Y = net(X)\n",
        "X.shape, Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffxQAQJkt9PU",
        "outputId": "a6559645-e113-41c3-965f-f135054f10d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "emb torch.Size([2, 5, 100])\n",
            "outputs torch.Size([2, 5, 100])\n",
            "type torch.Size([5, 2])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5, 2]), torch.Size([5, 2]))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    }
  ]
}