{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mxnet风格迁移.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhqKj6vne/35Ok6BYSmcP9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZhangYHe/MyColabDL_Repo/blob/main/mxnet%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-kC2ml5QGAL"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from mxnet import autograd, gluon, image, init, np, npx\n",
        "from mxnet.gluon import nn\n",
        "from d2l import mxnet as d2l\n",
        "\n",
        "npx.set_np()\n",
        "\n",
        "d2l.set_figsize()\n",
        "content_img = image.imread('../img/rainier.jpg')\n",
        "d2l.plt.imshow(content_img.asnumpy());"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "style_img = image.imread('../img/autumn-oak.jpg')\n",
        "d2l.plt.imshow(style_img.asnumpy());"
      ],
      "metadata": {
        "id": "BQtoIOI6SAiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rgb_mean = np.array([0.485, 0.456, 0.406])\n",
        "rgb_std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "def preprocess(img, image_shape):\n",
        "    img = image.imresize(img, *image_shape)\n",
        "    img = (img.astype('float32') / 255 - rgb_mean) / rgb_std\n",
        "    return np.expand_dims(img.transpose(2, 0, 1), axis=0)\n",
        "\n",
        "def postprocess(img):\n",
        "    img = img[0].as_in_ctx(rgb_std.ctx)\n",
        "    return (img.transpose(1, 2, 0) * rgb_std + rgb_mean).clip(0, 1)"
      ],
      "metadata": {
        "id": "YJaESDNCSC1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_net = gluon.model_zoo.vision.vgg19(pretrained=True)\n",
        "style_layers, content_layers = [0, 5, 10, 19, 28], [25]\n",
        "\n",
        "net = nn.Sequential()\n",
        "for i in range(max(content_layers + style_layers) + 1):\n",
        "    net.add(pretrained_net.features[i])"
      ],
      "metadata": {
        "id": "Pm8rO_HXSEMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(X, content_layers, style_layers):\n",
        "    contents = []\n",
        "    styles = []\n",
        "    for i in range(len(net)):\n",
        "        X = net[i](X)\n",
        "        if i in style_layers:\n",
        "            styles.append(X)\n",
        "        if i in content_layers:\n",
        "            contents.append(X)\n",
        "    return contents, styles"
      ],
      "metadata": {
        "id": "hzCAXwTsSJ_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_contents(image_shape, device):\n",
        "    content_X = preprocess(content_img, image_shape).copyto(device)\n",
        "    contents_Y, _ = extract_features(content_X, content_layers, style_layers)\n",
        "    return content_X, contents_Y\n",
        "\n",
        "def get_styles(image_shape, device):\n",
        "    style_X = preprocess(style_img, image_shape).copyto(device)\n",
        "    _, styles_Y = extract_features(style_X, content_layers, style_layers)\n",
        "    return style_X, styles_Y"
      ],
      "metadata": {
        "id": "UwFBV_ZMSLjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def content_loss(Y_hat, Y):\n",
        "    return np.square(Y_hat - Y).mean()\n",
        "\n",
        "def gram(X):\n",
        "    num_channels, n = X.shape[1], d2l.size(X) // X.shape[1]\n",
        "    X = X.reshape((num_channels, n))\n",
        "    return np.dot(X, X.T) / (num_channels * n)\n",
        "\n",
        "def style_loss(Y_hat, gram_Y):\n",
        "    return np.square(gram(Y_hat) - gram_Y).mean()\n",
        "\n",
        "def tv_loss(Y_hat):\n",
        "    return 0.5 * (np.abs(Y_hat[:, :, 1:, :] - Y_hat[:, :, :-1, :]).mean() +\n",
        "                  np.abs(Y_hat[:, :, :, 1:] - Y_hat[:, :, :, :-1]).mean())\n",
        "    \n",
        "content_weight, style_weight, tv_weight = 1, 1e3, 10\n",
        "\n",
        "def compute_loss(X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram):\n",
        "    # 分别计算内容损失、风格损失和全变分损失\n",
        "    contents_l = [content_loss(Y_hat, Y) * content_weight for Y_hat, Y in zip(\n",
        "        contents_Y_hat, contents_Y)]\n",
        "    styles_l = [style_loss(Y_hat, Y) * style_weight for Y_hat, Y in zip(\n",
        "        styles_Y_hat, styles_Y_gram)]\n",
        "    tv_l = tv_loss(X) * tv_weight\n",
        "    # 对所有损失求和\n",
        "    l = sum(10 * styles_l + contents_l + [tv_l])\n",
        "    return contents_l, styles_l, tv_l, l"
      ],
      "metadata": {
        "id": "uP0xjkqFSNAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SynthesizedImage(nn.Block):\n",
        "    def __init__(self, img_shape, **kwargs):\n",
        "        super(SynthesizedImage, self).__init__(**kwargs)\n",
        "        self.weight = self.params.get('weight', shape=img_shape)\n",
        "\n",
        "    def forward(self):\n",
        "        return self.weight.data()"
      ],
      "metadata": {
        "id": "rIgqq_veSXEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_inits(X, device, lr, styles_Y):\n",
        "    gen_img = SynthesizedImage(X.shape)\n",
        "    gen_img.initialize(init.Constant(X), ctx=device, force_reinit=True)\n",
        "    trainer = gluon.Trainer(gen_img.collect_params(), 'adam',\n",
        "                            {'learning_rate': lr})\n",
        "    styles_Y_gram = [gram(Y) for Y in styles_Y]\n",
        "    return gen_img(), styles_Y_gram, trainer"
      ],
      "metadata": {
        "id": "KQIp2qW_SZUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch):\n",
        "    X, styles_Y_gram, trainer = get_inits(X, device, lr, styles_Y)\n",
        "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
        "                            xlim=[10, num_epochs], ylim=[0, 20],\n",
        "                            legend=['content', 'style', 'TV'],\n",
        "                            ncols=2, figsize=(7, 2.5))\n",
        "    for epoch in range(num_epochs):\n",
        "        with autograd.record():\n",
        "            contents_Y_hat, styles_Y_hat = extract_features(\n",
        "                X, content_layers, style_layers)\n",
        "            contents_l, styles_l, tv_l, l = compute_loss(\n",
        "                X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)\n",
        "        l.backward()\n",
        "        trainer.step(1)\n",
        "        if (epoch + 1) % lr_decay_epoch == 0:\n",
        "            trainer.set_learning_rate(trainer.learning_rate * 0.8)\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            animator.axes[1].imshow(postprocess(X).asnumpy())\n",
        "            animator.add(epoch + 1, [float(sum(contents_l)),\n",
        "                                     float(sum(styles_l)), float(tv_l)])\n",
        "    return X"
      ],
      "metadata": {
        "id": "UPnEP_UiSawC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device, image_shape = d2l.try_gpu(), (450, 300)\n",
        "net.collect_params().reset_ctx(device)\n",
        "content_X, contents_Y = get_contents(image_shape, device)\n",
        "_, styles_Y = get_styles(image_shape, device)\n",
        "output = train(content_X, contents_Y, styles_Y, device, 0.9, 500, 50)"
      ],
      "metadata": {
        "id": "ZjBaCZE2ScXI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}